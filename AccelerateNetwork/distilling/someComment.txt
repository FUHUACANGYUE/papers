今天继续扯模型压缩的那些事，这篇我主要想扯下knowledge distilling，为方便之后简称KD，知乎关于KD的文章很多了，我就不详细介绍了，KD简单的说是想将复杂模型(teacher)的知识迁移到简单模型(student)中去，这样相当于在保持精度的同时减少了模型的复杂度，然后简单模型可以直接开跑，不需要像之前做量化那样做定点化了。

我觉得KD这个思想很好，但是怎么做就是八仙过海，各显神通了。我介绍下我试过的一些方法。

1.首先是hinton老爷子的论文，其思想是one-hot label这种方式没有考虑class之间的相关性，这样过于相信自己可能会造成过拟合的现象，所以就让teacher的输出作为label，然后将teacher和student的softmax输出除以系数T来减小预测值的输出范围，我的理解是这样做使得teacher和student的预测值更为接近，可以更容易拟合。但是T值不能太大，否则预测值都被压得太小了，就没有判决能力了呦。这种方法可以理解为KL散度，还有点类似于inception中的label smoothing的做法，label smoothing目的也是为了防止过拟合，两者的区别在于label smoothing中除了true label之外的label分配的概率是一样的，而该方法中label的概率是teacher的输出，这样做可以减少不相关的label的干扰。而且label smoothing不适用于class特别的多的情况，具体原因看下论文公式就明白了。这种方法也有很多缺点，首先是只适用于classfication+softmax，其他情况没法用；其次拟合最后的loss，如果前面的网络很深，loss不好往前传，这样拟合的效果也不好；再次如果class很多的话也不好拟合；还需要注意的地方是实际train的时候loss不能作为一个指标表明student网络是在变好。

2.然后我尝试了下fitnet，其思想是直接拟合中间层，然后再加上上面的softoutput cross entropy, 这样做相当于降低了拟合的难度，但是具体操作时效果却不尽如人意，因为sutdent和teacher的feature map的capacities就不一样，这样搞太苛刻了。或者我直接拟合下softamax之前的feature看看效果，结果我发现效果不错，甚至优于softoutput cross entropy，但是我又做了其他几个实验但是问题又出来了，如果teacher和student网络结构不类似的话，上述方法就失效了，所以直接拟合feature层还是有限制条件的。

3.之后我又尝试了rocket launching，其思想是teacher和student共享部分网络，同时一起训练，让teacher从头至尾一直指导student，loss是加权网络的cross entropy和两个网络的feature的regression，我跑了下作者的code发现下效果要优于hinton老爷子的方法，同时也说明和我之前的实验效果一致，但是我发现个问题就是teacher和student同时训练会导致teacher的精度降低，虽然作者加入了gradient block来缓解student对teacher的影响，但是这样一定程度还是会影响student提升的效果。

4.我最近又尝试了cvpr2017的一篇paper[1]，该论文思想与之前不同，是去拟合层与层之间的关系，利用gram矩阵去计算不同residual module的输入和输出的关系，然后teacher和student去做regression。打个比方，之前fitnet类似于做题时强调student和teacher做的题目最后结果要一样，而该论文相当于让student学习如何做题，相当于学习做题的套路。我同时又想到这种方法是不是和neural style有些类似，之前的fitnet的loss类似neural style中的content，而这片paper的loss类似neural style中的style，FSP矩阵其实也是一种Gram矩阵，只不过一个是不同层的feature map，一个是相同层的feature map。我还有个问题是这样做相对于fitnet是更容易拟合了吗？具体还要试验后才知道。

我发现做模型压缩本质上是在探究如何构造一个compact的网络，无论是从network,loss还是learning，而且我个人觉得在平时可以多关注一些其他领域诸如domain adaption等来对加深对模型压缩的思考

1.Junho Yim, Donggyu Joo, Ji-Hoon Bae, Junmo Kim. A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning